{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"n2GYQLkeyps1"},"outputs":[],"source":["# 下記セルを実行すると、Googleドライブへの接続を求められます。\n","# Googleアカウントにログインして進めてください。\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pIaZvWzyps1"},"outputs":[],"source":["working_dir = 'MLSys_100Knocks' #　※※自分で作成したフォルダパスが異なる場合こちらを変更してください。※※\n","path = f'/content/drive/MyDrive/{working_dir}/本章/9章'\n","os.chdir(path)"]},{"cell_type":"markdown","metadata":{"id":"eipYmMoVyps1"},"source":["# ９章_小規模機械学習システムを作成する１０本ノック\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r0pQdMHlyps2"},"source":["### ノック８１：フォルダ生成をして初期の変数定義をしよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ESeUjCrXyps2"},"outputs":[],"source":["import os\n","data_dir = 'data'\n","input_dir = os.path.join(data_dir, '00_input')\n","store_monthly_dir = os.path.join(data_dir, '01_store_monthly')\n","ml_base_dir = os.path.join(data_dir, '02_ml_base')\n","\n","output_ml_result_dir = os.path.join(data_dir, '10_output_ml_result')\n","output_report_dir = os.path.join(data_dir, '11_output_report')\n","\n","master_dir = os.path.join(data_dir, '99_master')\n","model_dir = 'models'\n","\n","os.makedirs(input_dir,exist_ok=True)\n","os.makedirs(store_monthly_dir,exist_ok=True)\n","os.makedirs(ml_base_dir,exist_ok=True)\n","os.makedirs(output_ml_result_dir,exist_ok=True)\n","os.makedirs(output_report_dir,exist_ok=True)\n","os.makedirs(master_dir,exist_ok=True)\n","os.makedirs(model_dir,exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"pWjJCn-Lyps2"},"source":["##### ＊＊必ずデータやモデルの配置をおこなってください。＊＊"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0wCjgwdyps2"},"outputs":[],"source":["tg_ym = '202004'\n","\n","target_file = \"tbl_order_\" + tg_ym + \".csv\"\n","m_area_file = 'm_area.csv'\n","m_store_file = 'm_store.csv'\n","store_monthly_file = 'store_monthly_data.csv'\n","ml_base_file = 'ml_base_data.csv'"]},{"cell_type":"markdown","metadata":{"id":"2lJa9HGTyps2"},"source":["### ノック８２：更新データを読み込んで店舗別データを作成しよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cBzM-JAyps2"},"outputs":[],"source":["import pandas as pd\n","m_area = pd.read_csv(os.path.join(master_dir, m_area_file))\n","m_store = pd.read_csv(os.path.join(master_dir, m_store_file))\n","target_data = pd.read_csv(os.path.join(input_dir, target_file))\n","\n","import datetime\n","max_date = pd.to_datetime(target_data[\"order_accept_date\"]).max()\n","min_date = pd.to_datetime(target_data[\"order_accept_date\"]).min()\n","max_str_date = max_date.strftime(\"%Y%m\")\n","min_str_date = min_date.strftime(\"%Y%m\")\n","if tg_ym == min_str_date and tg_ym == max_str_date:\n","    print(\"日付が一致しました\")\n","else:\n","    raise Exception(\"日付が一致しません\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gP4ipF8jyps3"},"outputs":[],"source":["def calc_delta(t):\n","    t1, t2 = t\n","    delta = t2 - t1\n","    return delta.total_seconds()/60\n","\n","def data_processing(order_data):\n","    order_data = order_data.loc[order_data['store_id'] != 999]\n","    order_data = pd.merge(order_data, m_store, on='store_id', how='left')\n","    order_data = pd.merge(order_data, m_area, on='area_cd', how='left')\n","    order_data.loc[order_data['takeout_flag'] == 0, 'takeout_name'] = 'デリバリー'\n","    order_data.loc[order_data['takeout_flag'] == 1, 'takeout_name'] = 'お持ち帰り'\n","    order_data.loc[order_data['status'] == 0, 'status_name'] = '受付'\n","    order_data.loc[order_data['status'] == 1, 'status_name'] = 'お支払済'\n","    order_data.loc[order_data['status'] == 2, 'status_name'] = 'お渡し済'\n","    order_data.loc[order_data['status'] == 9, 'status_name'] = 'キャンセル'\n","\n","    order_data.loc[:,'order_accept_datetime'] = pd.to_datetime(order_data['order_accept_date'])\n","    order_data.loc[:,'delivered_datetime'] = pd.to_datetime(order_data['delivered_date'])\n","    order_data.loc[:,'delta'] = order_data[['order_accept_datetime', 'delivered_datetime']].apply(calc_delta, axis=1)\n","    order_data.loc[:,'order_accept_hour'] = order_data['order_accept_datetime'].dt.hour\n","    order_data.loc[:,'order_accept_weekday'] = order_data['order_accept_datetime'].dt.weekday\n","    order_data.loc[order_data['order_accept_weekday'] >= 5, 'weekday_info'] = '休日'\n","    order_data.loc[order_data['order_accept_weekday'] < 5, 'weekday_info'] = '平日'\n","\n","    store_data = order_data.groupby(['store_name']).count()[['order_id']]\n","    store_f = order_data.loc[(order_data['status_name']==\"お渡し済\")|\n","                             (order_data['status_name']==\"お支払済\")].groupby(['store_name']).count()[['order_id']]\n","    store_c = order_data.loc[order_data['status_name']==\"キャンセル\"].groupby(['store_name']).count()[['order_id']]\n","    store_d = order_data.loc[order_data['takeout_name']==\"デリバリー\"].groupby(['store_name']).count()[['order_id']]\n","    store_t = order_data.loc[order_data['takeout_name']==\"お持ち帰り\"].groupby(['store_name']).count()[['order_id']]\n","    store_weekday = order_data.loc[order_data['weekday_info']==\"平日\"].groupby(['store_name']).count()[['order_id']]\n","    store_weekend = order_data.loc[order_data['weekday_info']==\"休日\"].groupby(['store_name']).count()[['order_id']]\n","    times = order_data['order_accept_hour'].unique()\n","    store_time = []\n","    for time in times:\n","        time_tmp = order_data.loc[order_data['order_accept_hour']==time].groupby(['store_name']).count()[['order_id']]\n","        time_tmp.columns = [f'order_time_{time}']\n","        store_time.append(time_tmp)\n","    store_time = pd.concat(store_time, axis=1)\n","    store_delta = order_data.loc[order_data['status_name']!=\"キャンセル\"].groupby(['store_name'])[['delta']].mean()\n","    store_data.columns = ['order']\n","    store_f.columns = ['order_fin']\n","    store_c.columns = ['order_cancel']\n","    store_d.columns = ['order_delivery']\n","    store_t.columns = ['order_takeout']\n","    store_delta.columns = ['delta_avg']\n","    store_weekday.columns = ['order_weekday']\n","    store_weekend.columns = ['order_weekend']\n","    store_data = pd.concat([store_data, store_f, store_c, store_d, store_t,\n","                        store_weekday, store_weekend, store_time, store_delta], axis=1)\n","    return store_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qxKF33z3yps3"},"outputs":[],"source":["store_data = data_processing(target_data)\n","store_data.reset_index(drop=False, inplace=True)\n","store_data.loc[:,'year_month'] = tg_ym\n","store_data.head(1)"]},{"cell_type":"markdown","metadata":{"id":"Qm4z0qTtyps3"},"source":["### ノック８３：月次店舗データの更新をしよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xJUZzfMyps4"},"outputs":[],"source":["store_monthly_data = pd.read_csv(os.path.join(store_monthly_dir, store_monthly_file))\n","print(f'更新前：{len(store_monthly_data)}件')\n","store_monthly_data = pd.concat([store_monthly_data, store_data], ignore_index=True)\n","store_monthly_data.loc[:, 'year_month'] = store_monthly_data['year_month'].astype(str)\n","store_monthly_data.drop_duplicates(subset=['store_name','year_month'], inplace=True, keep='last')\n","print(f'更新後：{len(store_monthly_data)}件')\n","store_monthly_data.to_csv(os.path.join(store_monthly_dir, store_monthly_file), index=False)"]},{"cell_type":"markdown","metadata":{"id":"Exv9GSv3yps4"},"source":["### ノック８４：機械学習用データの作成と更新をしよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejZTZMeVyps4"},"outputs":[],"source":["from dateutil.relativedelta import relativedelta\n","y = store_monthly_data[['store_name', 'year_month','order_weekday', 'order_weekend']].copy()\n","y.loc[:,'one_month_ago'] = pd.to_datetime(y['year_month'], format='%Y%m')\n","y.loc[:,'one_month_ago'] = y['one_month_ago'].map(lambda x: x - relativedelta(months=1))\n","y.loc[:,'one_month_ago'] = y['one_month_ago'].dt.strftime('%Y%m')\n","\n","y_one_month_ago = y.copy()\n","y_one_month_ago.rename(columns={'order_weekday':'order_weekday_one_month_ago',\n","                                'order_weekend':'order_weekend_one_month_ago',\n","                                'year_month':'year_month_for_join'}, inplace=True)\n","\n","y = pd.merge(y, y_one_month_ago[['store_name', 'year_month_for_join',\n","                                 'order_weekday_one_month_ago', 'order_weekend_one_month_ago']],\n","                                 left_on=['store_name', 'one_month_ago'],\n","                                 right_on=['store_name','year_month_for_join'], how='left')\n","\n","y.dropna(inplace=True)\n","y.loc[y['order_weekday'] - y['order_weekday_one_month_ago'] > 0, 'y_weekday'] = 1\n","y.loc[y['order_weekday'] - y['order_weekday_one_month_ago'] <= 0, 'y_weekday'] = 0\n","y.loc[y['order_weekend'] - y['order_weekend_one_month_ago'] > 0, 'y_weekend'] = 1\n","y.loc[y['order_weekend'] - y['order_weekend_one_month_ago'] <= 0, 'y_weekend'] = 0\n","\n","y.rename(columns={'year_month':'target_year_month'},inplace=True)\n","y = y[['store_name','target_year_month', 'one_month_ago', 'y_weekday', 'y_weekend']].copy()\n","ml_data = pd.merge(y, store_monthly_data, left_on=['store_name','one_month_ago'],\n","                   right_on=['store_name','year_month'], how='left')\n","\n","del ml_data[\"target_year_month\"]\n","del ml_data[\"one_month_ago\"]\n","ml_data.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rq-1wsT2yps4"},"outputs":[],"source":["ml_base_data = pd.read_csv(os.path.join(ml_base_dir, ml_base_file))\n","print(f'更新前：{len(ml_base_data)}件')\n","ml_base_data = pd.concat([ml_base_data, ml_data], ignore_index=True)\n","ml_base_data.loc[:, 'year_month'] = ml_base_data['year_month'].astype(str)\n","ml_base_data.drop_duplicates(subset=['store_name','year_month'], inplace=True, keep='last')\n","print(f'更新後：{len(ml_base_data)}件')\n","ml_base_data.to_csv(os.path.join(ml_base_dir, ml_base_file), index=False)"]},{"cell_type":"markdown","metadata":{"id":"m9z6oyJ6yps4"},"source":["### ノック８５：機械学習モデル用の事前データ加工をしよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vocO54cIyps4"},"outputs":[],"source":["category_data = pd.get_dummies(ml_base_data['store_name'], prefix='store' ,prefix_sep='_')\n","del category_data['store_麻生店']\n","del ml_base_data['year_month']\n","del ml_base_data['store_name']\n","ml_base_data = pd.concat([ml_base_data, category_data],axis=1)\n","\n","from sklearn.model_selection import train_test_split\n","train_data, test_data = train_test_split(ml_base_data, test_size=0.3, random_state=0)\n","print(f'Train：{len(train_data)}件/ Test:{len(test_data)}')\n","print(f'Weekday Train0：{len(train_data.loc[train_data[\"y_weekday\"]==0])}件')\n","print(f'Weekday Train1：{len(train_data.loc[train_data[\"y_weekday\"]==1])}件')\n","print(f'Weekday Test0：{len(test_data.loc[test_data[\"y_weekday\"]==0])}件')\n","print(f'Weekday Test1：{len(test_data.loc[test_data[\"y_weekday\"]==1])}件')\n","\n","print(f'Weekend Train0：{len(train_data.loc[train_data[\"y_weekend\"]==0])}件')\n","print(f'Weekend Train1：{len(train_data.loc[train_data[\"y_weekend\"]==1])}件')\n","print(f'Weekend Test0：{len(test_data.loc[test_data[\"y_weekend\"]==0])}件')\n","print(f'Weekend Test1：{len(test_data.loc[test_data[\"y_weekend\"]==1])}件')"]},{"cell_type":"markdown","metadata":{"id":"YWbM2iVPyps4"},"source":["### ノック８６：機械学習モデルの構築・評価をしよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8MEh4pZyps5"},"outputs":[],"source":["def make_model_and_eval(model, X_train, X_test, y_train, y_test):\n","    model.fit(X_train, y_train)\n","    y_pred_train = model.predict(X_train)\n","    y_pred_test = model.predict(X_test)\n","\n","    acc_train = accuracy_score(y_train, y_pred_train)\n","    acc_test = accuracy_score(y_test, y_pred_test)\n","    f1_train = f1_score(y_train, y_pred_train)\n","    f1_test = f1_score(y_test, y_pred_test)\n","    recall_train = recall_score(y_train, y_pred_train)\n","    recall_test = recall_score(y_test, y_pred_test)\n","    precision_train = precision_score(y_train, y_pred_train)\n","    precision_test = precision_score(y_test, y_pred_test)\n","    tn_train, fp_train, fn_train, tp_train = confusion_matrix(y_train, y_pred_train).ravel()\n","    tn_test, fp_test, fn_test, tp_test = confusion_matrix(y_test, y_pred_test).ravel()\n","    score_train = pd.DataFrame({'DataCategory':['train'],'acc':[acc_train],'f1':[f1_train],\n","                                'recall':[recall_train],'precision':[precision_train],\n","                                'tp':[tp_train],'fn':[fn_train],'fp':[fp_train],'tn':[tn_train]})\n","    score_test = pd.DataFrame({'DataCategory':['test'], 'acc':[acc_test],'f1':[f1_test],\n","                                'recall':[recall_test],'precision':[precision_test],\n","                                'tp':[tp_test],'fn':[fn_test],'fp':[fp_test],'tn':[tn_test]})\n","    score = pd.concat([score_train,score_test], ignore_index=True)\n","    importance = pd.DataFrame({'cols':X_train.columns, 'importance':model.feature_importances_})\n","    importance = importance.sort_values('importance', ascending=False)\n","    cols = pd.DataFrame({'X_cols':X_train.columns})\n","    display(score)\n","    return score, importance, model, cols"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tVZjcC3pyps5"},"outputs":[],"source":["from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score,confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","import pickle\n","\n","X_cols = list(train_data.columns)\n","X_cols.remove('y_weekday')\n","X_cols.remove('y_weekend')\n","targets_y = ['y_weekday', 'y_weekend']\n","\n","target_output_dir_name = f'results_{tg_ym}'\n","target_output_dir = os.path.join(output_ml_result_dir, target_output_dir_name)\n","os.makedirs(target_output_dir, exist_ok=True)\n","print(target_output_dir)\n","\n","score_all = []\n","importance_all = []\n","\n","for target_y in targets_y:\n","    y_train = train_data[target_y]\n","    X_train = train_data[X_cols]\n","    y_test = test_data[target_y]\n","    X_test = test_data[X_cols]\n","\n","    models = {'tree': DecisionTreeClassifier(random_state=0),\n","              'RandomForest':RandomForestClassifier(random_state=0),\n","              'GradientBoosting':GradientBoostingClassifier(random_state=0)}\n","\n","    for model_name, model in models.items():\n","        print(model_name)\n","        score, importance, model, cols = make_model_and_eval(model, X_train, X_test, y_train, y_test)\n","        score['model_name'] = model_name\n","        importance['model_name'] = model_name\n","        score['model_target'] = target_y\n","        importance['model_target'] = target_y\n","\n","        model_nema = f'model_{target_y}_{model_name}.pickle'\n","        model_path = os.path.join(target_output_dir, model_nema)\n","        with open(model_path, mode='wb') as f:\n","            pickle.dump(model, f, protocol=2)\n","        score_all.append(score)\n","        importance_all.append(importance)\n","\n","score_all = pd.concat(score_all, ignore_index=True)\n","importance_all = pd.concat(importance_all, ignore_index=True)\n","cols = pd.DataFrame({'X_cols':X_train.columns})\n","\n","score_name = 'score.csv'\n","importance_name = 'importance.csv'\n","cols_name = 'X_cols.csv'\n","score_path = os.path.join(target_output_dir, score_name)\n","importance_path = os.path.join(target_output_dir, importance_name)\n","cols_path = os.path.join(target_output_dir, cols_name)\n","score_all.to_csv(score_path, index=False)\n","importance_all.to_csv(importance_path, index=False)\n","cols.to_csv(cols_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"DpUG6GmFyps5"},"source":["### ノック８７：新規データ予測に向けた下準備をしよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3p-Ranzyps5"},"outputs":[],"source":["category_data = pd.get_dummies(store_data['store_name'], prefix='store' ,prefix_sep='_')\n","del category_data['store_麻生店']\n","store_data = pd.concat([store_data, category_data],axis=1)\n","\n","X_cols_name = 'X_cols.csv'\n","X_cols = pd.read_csv(os.path.join(model_dir, X_cols_name))\n","X_cols = X_cols['X_cols']\n","\n","X = store_data[X_cols].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lrAB4Ydiyps5"},"outputs":[],"source":["model_weekday_name = 'model_y_weekday_GradientBoosting.pickle'\n","model_weekend_name = 'model_y_weekend_GradientBoosting.pickle'\n","\n","model_weekday_path = os.path.join(model_dir, model_weekday_name)\n","model_weekend_path = os.path.join(model_dir, model_weekend_name)\n","\n","with open(model_weekday_path, mode='rb') as f:\n","    model_weekday = pickle.load(f)\n","\n","with open(model_weekend_path, mode='rb') as f:\n","    model_weekend = pickle.load(f)"]},{"cell_type":"markdown","metadata":{"id":"TLBBuoZnyps5"},"source":["### ノック８８：新規データの予測をしよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cO3fYYGSyps5"},"outputs":[],"source":["pred_weekday = model_weekday.predict(X)\n","pred_weekend = model_weekend.predict(X)\n","pred_proba_weekday = model_weekday.predict_proba(X)[:,1]\n","pred_proba_weekend = model_weekend.predict_proba(X)[:,1]\n","pred = pd.DataFrame({'pred_weekday':pred_weekday, 'pred_weekend':pred_weekend,\n","                     'score_weekday':pred_proba_weekday, 'score_weekend':pred_proba_weekend})\n","pred.loc[:,'store_name'] = store_data['store_name']\n","pred.loc[:,'year_month'] = tg_ym\n","pred.head(3)"]},{"cell_type":"markdown","metadata":{"id":"q0yhRY17yps6"},"source":["### ノック８９：現場向けレポートを作成し出力しよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8DOkFxL7yps6"},"outputs":[],"source":["target_cols = ['store_name', 'order', 'order_fin', 'order_cancel', 'order_delivery',\n","       'order_takeout', 'order_weekday', 'order_weekend', 'delta_avg']\n","store_data = store_data[target_cols]\n","actual_cols = ['store_name']\n","rename_cols = [x + f'_{tg_ym}' for x in store_data.columns if x != 'store_name']\n","actual_cols.extend(rename_cols)\n","store_data.columns = actual_cols\n","store_data.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FwAWfTytyps6"},"outputs":[],"source":["pred.loc[pred['score_weekday'] >= 0.75,'オーダー予測 平日'] = '増加大'\n","pred.loc[(pred['score_weekday'] < 0.75)&(pred['score_weekday'] >= 0.5),'オーダー予測 平日'] = '増加'\n","pred.loc[(pred['score_weekday'] < 0.5)&(pred['score_weekday'] >= 0.25),'オーダー予測 平日'] = '減少'\n","pred.loc[pred['score_weekday'] < 0.25,'オーダー予測 平日'] = '減少大'\n","\n","pred.loc[pred['score_weekend'] >= 0.75,'オーダー予測 休日'] = '増加大'\n","pred.loc[(pred['score_weekend'] < 0.75)&(pred['score_weekend'] >= 0.5),'オーダー予測 休日'] = '増加'\n","pred.loc[(pred['score_weekend'] < 0.5)&(pred['score_weekend'] >= 0.25),'オーダー予測 休日'] = '減少'\n","pred.loc[pred['score_weekend'] < 0.25,'オーダー予測 休日'] = '減少大'\n","\n","report = pred[['store_name','オーダー予測 平日','オーダー予測 休日', 'score_weekday', 'score_weekend']]\n","report = pd.merge(report, store_data , on='store_name', how='left')\n","\n","pred_ym = datetime.datetime.strptime(tg_ym, '%Y%m')\n","from dateutil.relativedelta import relativedelta\n","pred_ym = pred_ym + relativedelta(months=1)\n","pred_ym = datetime.datetime.strftime(pred_ym, '%Y%m')\n","\n","report_name = f'report_pred_{pred_ym}.xlsx'\n","print(report_name)\n","report.to_excel(os.path.join(output_report_dir, report_name), index=False)"]},{"cell_type":"markdown","metadata":{"id":"UMX80MHkyps6"},"source":["#### ＊＊次ノックに行く前に、ノック81のtg_ymを、202005、202006、202007、202008の順番に指定し、実行してください。＊＊"]},{"cell_type":"markdown","metadata":{"id":"KC44nzL9yps6"},"source":["### ノック９０：機械学習モデルの精度推移を可視化しよう"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CRnc5b4yps6"},"outputs":[],"source":["ml_results_dirs = os.listdir(output_ml_result_dir)\n","score_all = []\n","for ml_results_dir in ml_results_dirs:\n","    score_file_path = os.path.join(output_ml_result_dir,ml_results_dir, 'score.csv')\n","    score_monthly = pd.read_csv(score_file_path)\n","    score_monthly['dirs'] = ml_results_dir\n","    score_all.append(score_monthly)\n","score_all = pd.concat(score_all,ignore_index=True)\n","score_all.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-qdHaRUlyps6"},"outputs":[],"source":["score_all_gb = score_all.loc[(score_all['model_name']=='GradientBoosting')&(score_all['DataCategory']=='test')]\n","model_targets = score_all_gb['model_target'].unique()\n","import matplotlib.pyplot as plt\n","\n","for model_target in model_targets:\n","    view_data = score_all_gb.loc[score_all_gb['model_target']==model_target]\n","    plt.scatter(view_data['dirs'], view_data['acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_IwlIrThyps6"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}